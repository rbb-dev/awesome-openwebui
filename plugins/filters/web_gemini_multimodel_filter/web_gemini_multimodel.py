"""
title: Gemini å¤šæ¨¡æ€è¿‡æ»¤å™¨ï¼ˆå«å­—å¹•å¢å¼ºï¼‰
author: Gemini Adapter
author_url: https://github.com/Fu-Jie
funding_url: https://github.com/Fu-Jie/awesome-openwebui
version: 0.3.2
description: >
    ä¸€ä¸ªå¼ºå¤§çš„è¿‡æ»¤å™¨ï¼Œä¸º OpenWebUI ä¸­çš„ä»»ä½•æ¨¡å‹æä¾›å¤šæ¨¡æ€èƒ½åŠ›ï¼šPDFã€Officeã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰ã€‚
    é»˜è®¤æ™ºèƒ½è·¯ç”±è‡³ Gemini è¿›è¡Œåˆ†æ/ç›´è¿ï¼›å½“æ£€æµ‹åˆ°â€œè§†é¢‘+å­—å¹•â€éœ€æ±‚æ—¶ï¼Œä¼šè‡ªåŠ¨å¯ç”¨å­—å¹•ç²¾ä¿®ä¸“å®¶ç”Ÿæˆé«˜è´¨é‡ SRT ä½œä¸ºä¸Šä¸‹æ–‡ã€‚

åŠŸèƒ½ç‰¹æ€§:
- **å¤šæ¨¡æ€æ”¯æŒ**: å¤„ç† PDF, Word, Excel, PowerPoint, EPUB, MP3, MP4 å’Œå›¾ç‰‡ã€‚
- **æ™ºèƒ½è·¯ç”±**:
    - **ç›´è¿æ¨¡å¼ (Direct Mode)**: å¯¹äº Gemini æ¨¡å‹ï¼Œæ–‡ä»¶ç›´æ¥ä¼ é€’ï¼ˆåŸç”Ÿå¤šæ¨¡æ€ï¼‰ã€‚
    - **åˆ†æå™¨æ¨¡å¼ (Analyzer Mode)**: å¯¹äºé Gemini æ¨¡å‹ï¼ˆå¦‚ DeepSeek, Llamaï¼‰ï¼Œæ–‡ä»¶ç”± Gemini åˆ†æï¼Œç»“æœæ³¨å…¥ä¸ºä¸Šä¸‹æ–‡ã€‚
- **æŒä¹…ä¸Šä¸‹æ–‡**: åˆ©ç”¨ OpenWebUI çš„ Chat ID è·¨å¤šè½®å¯¹è¯ç»´æŠ¤ä¼šè¯å†å²ã€‚
- **æ•°æ®åº“å»é‡**: è‡ªåŠ¨è®°å½•å·²åˆ†ææ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œé˜²æ­¢é‡å¤ä¸Šä¼ å’Œåˆ†æï¼ŒèŠ‚çœèµ„æºã€‚
- **æ™ºèƒ½è¿½é—®**: æ”¯æŒé’ˆå¯¹å·²ä¸Šä¼ æ–‡æ¡£çš„çº¯æ–‡æœ¬è¿½é—®ï¼Œè‡ªåŠ¨è°ƒç”¨ Gemini è¿›è¡Œä¸Šä¸‹æ–‡åˆ†æã€‚

é…ç½®é¡¹ (Valves):
- `gemini_adapter_url`: Gemini Adapter æœåŠ¡çš„ URLã€‚
- `target_model_keyword`: ç”¨äºè¯†åˆ« Gemini æ¨¡å‹çš„å…³é”®å­—ï¼ˆé»˜è®¤: "webgemini"ï¼‰ã€‚
- `mode`: "auto" (æ¨è), "direct" (ç›´è¿), æˆ– "analyzer" (åˆ†æå™¨)ã€‚
- `analyzer_base_model_id`: ç”¨äºæ–‡æ¡£åˆ†æçš„åŸºç¡€ Gemini æ¨¡å‹ï¼ˆé»˜è®¤: "gemini-3.0-pro"ï¼‰ã€‚
"""

import os
import asyncio
import requests
import base64
import mimetypes
import time
from typing import List, Union, Generator, Iterator, Optional
from pydantic import BaseModel, Field
import sqlalchemy
from sqlalchemy import (
    create_engine,
    Column,
    String,
    Text,
    DateTime,
    Integer,
    Boolean,
    JSON,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from datetime import datetime

Base = declarative_base()


class GeminiAnalysisHistory(Base):
    """è®°å½•å·²åˆ†ææ–‡ä»¶çš„å†å²ï¼Œç”¨äºå»é‡"""

    __tablename__ = "gemini_analysis_history"

    id = Column(Integer, primary_key=True, autoincrement=True)
    chat_id = Column(String(255), nullable=False, index=True)
    file_hash = Column(String(255), nullable=False, index=True)
    file_id = Column(String(255), nullable=True)  # è®°å½•å½“æ—¶çš„ file_id ä»…ä¾›å‚è€ƒ
    filename = Column(String(255), nullable=True)
    analyzed_at = Column(DateTime, default=datetime.utcnow)


class File(Base):
    """OpenWebUI ç°æœ‰çš„ file è¡¨æ˜ å°„ (åªè¯»)"""

    __tablename__ = "file"
    id = Column(String, primary_key=True)
    hash = Column(String)
    filename = Column(String)

    # å…¶ä»–å­—æ®µæˆ‘ä»¬æš‚æ—¶ä¸éœ€è¦


class Filter:
    class Valves(BaseModel):
        priority: int = Field(
            default=8, description="Priority level for the filter operations."
        )
        gemini_adapter_url: str = Field(
            default="http://192.168.31.19:8197",
            description="URL of the Gemini Adapter",
        )
        openwebui_upload_path: str = Field(
            default="/app/backend/data/uploads",
            description="Path to OpenWebUI uploads directory (mapped in Docker)",
        )
        supported_extensions: str = Field(
            default=".pdf, .doc, .docx, .xls, .xlsx, .ppt, .pptx, .epub, .mp3, .wav, .mp4, .mov, .avi, .mkv, .webm",
            description="Comma-separated list of supported file extensions",
        )
        target_model_keyword: str = Field(
            default="webgemini",
            description="Keyword to identify Gemini models (e.g., 'webgemini')",
        )
        mode: str = Field(
            default="auto",
            description="Operation mode: 'auto' (decide based on model), 'direct' (pass file URL), or 'analyzer' (analyze first)",
        )
        analyzer_base_model_id: str = Field(
            default="gemini-3.0-pro",
            description="Base Model ID to use for document analysis (Brain)",
        )
        analyzer_custom_model_id: str = Field(
            default="gemini-analyzer",
            description="Custom model ID for general document analysis (Gem)",
        )
        subtitle_custom_model_id: str = Field(
            default="subtitle-master",
            description="Custom model ID for subtitle extraction / SRT polishing",
        )
        subtitle_keywords: str = Field(
            default="å­—å¹•,å­—å¹•ç²¾ä¿®,å­—å¹•æå–,srt",
            description="Comma-separated keywords to trigger subtitle flow when paired with video uploads",
        )
        subtitle_video_extensions: str = Field(
            default=".mp4, .mov, .avi, .mkv, .webm",
            description="Video extensions that will trigger subtitle flow when keywords are present",
        )
        upload_timeout_seconds: int = Field(
            default=600, description="Timeout for adapter upload requests (seconds)"
        )
        analyze_timeout_seconds: int = Field(
            default=600, description="Timeout for adapter analysis requests (seconds)"
        )
        max_retries: int = Field(
            default=2, description="Max retry attempts for adapter calls"
        )
        retry_backoff_seconds: float = Field(
            default=60.0, description="Backoff seconds between retries"
        )
        circuit_failure_threshold: int = Field(
            default=3,
            description="Number of consecutive failures before opening circuit",
        )
        circuit_reset_seconds: int = Field(
            default=60, description="Cooldown seconds before closing circuit after trip"
        )

    def __init__(self):
        self.valves = self.Valves()
        self._db_engine = None
        self._SessionLocal = None
        self._init_database()
        self._failure_count = 0
        self._circuit_open_until = 0

    def _circuit_open(self) -> bool:
        now = time.time()
        if now < self._circuit_open_until:
            return True
        return False

    def _record_success(self):
        self._failure_count = 0
        self._circuit_open_until = 0

    def _record_failure(self):
        self._failure_count += 1
        if self._failure_count >= self.valves.circuit_failure_threshold:
            self._circuit_open_until = time.time() + self.valves.circuit_reset_seconds
            print(
                f"ğŸš§ Circuit open for adapter calls ({self.valves.circuit_reset_seconds}s) after {self._failure_count} failures"
            )

    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å’Œè¡¨"""
        try:
            database_url = os.getenv("DATABASE_URL")
            if not database_url:
                print(
                    "âš ï¸ DATABASE_URL not set. Deduplication will fall back to memory (unreliable)."
                )
                return

            # Handle postgres protocol
            if database_url.startswith("postgres://"):
                database_url = database_url.replace("postgres://", "postgresql://", 1)

            self._db_engine = create_engine(database_url, pool_pre_ping=True)
            self._SessionLocal = sessionmaker(
                autocommit=False, autoflush=False, bind=self._db_engine
            )

            # Create our history table if it doesn't exist
            Base.metadata.create_all(bind=self._db_engine)
            print("âœ… Gemini Filter: Database initialized and tables checked.")
        except Exception as e:
            print(f"âŒ Gemini Filter: Database initialization failed: {e}")
            self._db_engine = None

    def get_db(self):
        if not self._SessionLocal:
            return None
        return self._SessionLocal()

    def get_file_hash(self, file_id: str) -> Optional[str]:
        """ä» OpenWebUI çš„ file è¡¨è·å–æ–‡ä»¶å“ˆå¸Œ"""
        db = self.get_db()
        if not db:
            return None
        try:
            file_record = db.query(File).filter(File.id == file_id).first()
            return file_record.hash if file_record else None
        except Exception as e:
            print(f"âš ï¸ Failed to query file hash: {e}")
            return None
        finally:
            db.close()

    def is_file_analyzed(self, chat_id: str, file_hash: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²åœ¨å½“å‰ä¼šè¯ä¸­åˆ†æè¿‡"""
        db = self.get_db()
        if not db:
            return False
        try:
            record = (
                db.query(GeminiAnalysisHistory)
                .filter(
                    GeminiAnalysisHistory.chat_id == chat_id,
                    GeminiAnalysisHistory.file_hash == file_hash,
                )
                .first()
            )
            return record is not None
        except Exception as e:
            print(f"âš ï¸ Failed to check analysis history: {e}")
            return False
        finally:
            db.close()

    def mark_file_analyzed(
        self, chat_id: str, file_hash: str, file_id: str, filename: str
    ):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²åˆ†æ"""
        db = self.get_db()
        if not db:
            return
        try:
            new_record = GeminiAnalysisHistory(
                chat_id=chat_id, file_hash=file_hash, file_id=file_id, filename=filename
            )
            db.add(new_record)
            db.commit()
            print(
                f"ğŸ’¾ Marked file {filename} (hash: {file_hash[:8]}...) as analyzed in chat {chat_id}"
            )
        except Exception as e:
            print(f"âš ï¸ Failed to mark file as analyzed: {e}")
            db.rollback()
        finally:
            db.close()

    def has_analyzed_files_in_chat(self, chat_id: str) -> bool:
        """æ£€æŸ¥å½“å‰ä¼šè¯æ˜¯å¦æœ‰ä»»ä½•å·²åˆ†æçš„æ–‡ä»¶ï¼ˆç”¨äºè¿½é—®åˆ¤æ–­ï¼‰"""
        db = self.get_db()
        if not db:
            return False
        try:
            record = (
                db.query(GeminiAnalysisHistory)
                .filter(GeminiAnalysisHistory.chat_id == chat_id)
                .first()
            )
            return record is not None
        except Exception as e:
            return False
        finally:
            db.close()

    def upload_to_adapter(
        self, file_content: bytes, filename: str, content_type: str
    ) -> str:
        """
        Uploads file content to Gemini Adapter and returns the accessible URL.
        """
        if self._circuit_open():
            print("ğŸš« Upload circuit open, skipping upload")
            return None

        upload_url = f"{self.valves.gemini_adapter_url}/v1/files/upload"
        timeout = self.valves.upload_timeout_seconds
        retries = self.valves.max_retries
        backoff = self.valves.retry_backoff_seconds

        print(
            f"ğŸ“¤ Uploading {filename} to {upload_url} (timeout={timeout}s, retries={retries})..."
        )

        for attempt in range(retries + 1):
            try:
                files = {"file": (filename, file_content, content_type)}
                response = requests.post(upload_url, files=files, timeout=timeout)
                response.raise_for_status()

                result = response.json()
                file_url = result.get("url")
                print(f"âœ… Upload success: {file_url}")
                self._record_success()
                return file_url
            except Exception as e:
                print(f"âŒ Upload attempt {attempt+1} failed: {e}")
                self._record_failure()
                if attempt < retries:
                    time.sleep(backoff)
                else:
                    print("ğŸš« Upload exhausted retries")
        return None

    def analyze_document(
        self,
        file_url: Optional[str],
        user_message: str,
        openwebui_chat_id: Optional[str] = None,
        custom_model_id: Optional[str] = None,
        subtitle_mode: bool = False,
    ) -> Optional[str]:
        """
        Send file URL and user message to Gemini Adapter for analysis or subtitle extraction.
        Uses OpenWebUI's chat_id directly, relying on Adapter's database for context persistence.
        """
        print(
            f"ğŸ§  Analyzing/Chatting with Gemini. File: {file_url}, ChatID: {openwebui_chat_id}, SubtitleMode: {subtitle_mode}"
        )

        headers = {
            "Content-Type": "application/json",
        }

        chat_url = f"{self.valves.gemini_adapter_url}/v1/chat/completions"
        timeout = self.valves.analyze_timeout_seconds
        retries = self.valves.max_retries
        backoff = self.valves.retry_backoff_seconds

        if self._circuit_open():
            print("ğŸš« Analyze circuit open, skipping analysis")
            return None

        # Construct Prompt
        # If file_url is present, it's likely the first turn or a new file.
        # If file_url is None, it's a follow-up question.
        if subtitle_mode:
            base_prompt = (
                "ä½ æ˜¯èµ„æ·±çš„å­—å¹•ç²¾ä¿®ä¸“å®¶ï¼Œè´Ÿè´£å°†ä¸Šä¼ çš„éŸ³è§†é¢‘å†…å®¹è½¬å†™å¹¶æ‰“ç£¨ä¸ºé«˜è´¨é‡çš„ SRT å­—å¹•ã€‚"
                "åŠ¡å¿…ä¿ç•™æŠ€æœ¯åè¯ï¼Œåˆ é™¤å£ç™–ï¼Œä¿è¯åŒè¯­æ··æ’ç©ºæ ¼è§„èŒƒã€‚"
                "æ—¶é—´è½´éœ€è¦è¿ç»­ã€å‡†ç¡®ï¼Œæ–‡æœ¬å»ºè®®å•è¡Œä¸è¶…è¿‡ 20 ä¸ªä¸­æ–‡å­—ç¬¦ã€‚"
            )
            if file_url:
                prompt = (
                    f"{base_prompt}\n\nè¯·è†å¬/é˜…è¯»é™„ä»¶å†…å®¹ï¼Œå¹¶ä¾æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆæ ‡å‡† SRTï¼š\n"
                    f"ç”¨æˆ·é—®é¢˜ï¼š{user_message}"
                )
                content_list = [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": file_url}},
                ]
            else:
                prompt = (
                    f"{base_prompt}\n\nåŸºäºå‰æ–‡ä¸Šä¸‹æ–‡ï¼Œç»§ç»­å›ç­”ç”¨æˆ·çš„æ–°éœ€æ±‚ï¼š{user_message}\n"
                    "è‹¥éœ€è¦è¡¥å…¨æˆ–ä¿®è®¢å·²æœ‰å­—å¹•ï¼Œè¯·ä¿æŒæ—¶é—´è½´è¿ç»­ã€‚"
                )
                content_list = [{"type": "text", "text": prompt}]
        else:
            if file_url:
                prompt = f"è¯·é˜…è¯»é™„ä»¶æ–‡æ¡£ï¼Œå¹¶é’ˆå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œè¯¦ç»†åˆ†æå’Œå›ç­”ï¼š\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}\n\nè¯·æä¾›è¯¦å°½çš„åˆ†æç»“æœï¼Œè¿™å°†ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™å¦ä¸€ä¸ª AI æ¨¡å‹ã€‚"
                content_list = [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": file_url}},
                ]
            else:
                # Follow-up question (no file needed, context is in session)
                prompt = f"åŸºäºä¹‹å‰çš„æ–‡æ¡£å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„æ–°é—®é¢˜ï¼š{user_message}\n\nè¯·æä¾›è¯¦å°½çš„å›ç­”ï¼Œè¿™å°†ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™å¦ä¸€ä¸ª AI æ¨¡å‹ã€‚"
                content_list = [{"type": "text", "text": prompt}]

        # Construct request payload
        payload = {
            "model": self.valves.analyzer_base_model_id,
            "custom_model_id": custom_model_id or self.valves.analyzer_custom_model_id,
            "chat_id": openwebui_chat_id,  # Pass OpenWebUI chat_id directly
            "messages": [
                {
                    "role": "user",
                    "content": content_list,
                }
            ],
            "stream": False,
        }

        persona = custom_model_id or self.valves.analyzer_custom_model_id
        print(
            f"ğŸ§  Analyzing document with {self.valves.analyzer_base_model_id} (Persona: {persona})..."
        )
        for attempt in range(retries + 1):
            try:
                response = requests.post(chat_url, json=payload, timeout=timeout)
                response.raise_for_status()
                result = response.json()
                analysis = result["choices"][0]["message"]["content"]
                print(f"âœ… Analysis complete ({len(analysis)} chars)")
                self._record_success()
                return analysis
            except Exception as e:
                print(f"âŒ Analysis attempt {attempt+1} failed: {e}")
                self._record_failure()
                if attempt < retries:
                    time.sleep(backoff)
                else:
                    print("ğŸš« Analysis exhausted retries")
        return None

    def process_url(self, url: str, should_process_images: bool = True) -> str:
        """
        Process a URL (Base64 or HTTP), upload to adapter if needed, and return new URL.
        """
        # 1. Handle Base64
        if url.startswith("data:"):
            try:
                header, encoded = url.split(",", 1)
                mime_type = header.split(";")[0].split(":")[1]

                # Check if it's an image and if we should process it
                is_image = "image" in mime_type
                if is_image and not should_process_images:
                    return url

                # Only process PDF, images, audio, or video (if allowed)
                if (
                    "pdf" not in mime_type
                    and not is_image
                    and "audio" not in mime_type
                    and "video" not in mime_type
                ):
                    return url

                # Decode
                file_content = base64.b64decode(encoded)
                extension = mimetypes.guess_extension(mime_type) or ".bin"
                filename = f"upload{extension}"

                # Upload
                new_url = self.upload_to_adapter(file_content, filename, mime_type)
                return new_url if new_url else url

            except Exception as e:
                print(f"âŒ Error processing Base64: {e}")
                return url
        return url

    async def emit_status(
        self, __event_emitter__, description: str, done: bool = False
    ):
        if __event_emitter__:
            await __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": description,
                        "done": done,
                    },
                }
            )

    async def inlet(
        self,
        body: dict,
        __event_emitter__=None,
        __user__: Optional[dict] = None,
        __model__: Optional[dict] = None,
        __metadata__: Optional[dict] = None,
    ) -> dict:
        print(f"ğŸ” Gemini PDF Filter: Inlet triggered")

        # Extract Chat ID from metadata
        openwebui_chat_id = None
        if __metadata__:
            openwebui_chat_id = __metadata__.get("chat_id")
            print(f"ğŸ†” OpenWebUI Chat ID: {openwebui_chat_id}")

        # 1. Determine the actual model ID (handling custom models)
        base_model_id = None
        if __model__:
            if "openai" in __model__:
                base_model_id = __model__["openai"].get("id")
            else:
                base_model_id = __model__.get("info", {}).get("base_model_id")

        # Fallback to body['model'] if base_model_id not found or __model__ is missing
        current_model = base_model_id if base_model_id else body.get("model", "")

        print(
            f"ğŸ¤– Checking model: {current_model} (Target: {self.valves.target_model_keyword})"
        )

        # Check if model matches target keyword
        is_target_model = (
            self.valves.target_model_keyword.lower() in current_model.lower()
        )

        # Determine effective mode
        effective_mode = self.valves.mode
        if effective_mode == "auto":
            if is_target_model:
                effective_mode = "direct"
            else:
                effective_mode = "analyzer"

        print(f"âš™ï¸ Effective Mode: {effective_mode}")

        # If not target model AND effective mode is NOT analyzer, skip.
        # This means if we are in 'direct' mode (forced or auto), we only run for target models.
        if not is_target_model and effective_mode != "analyzer":
            return body

        messages = body.get("messages", [])
        if not messages:
            return body

        last_message = messages[-1]
        if last_message.get("role") != "user":
            return body

        # Extract plain text from the last user message for intent detection
        user_text_plain = last_message.get("content", "")
        if isinstance(user_text_plain, list):
            user_text_plain = " ".join(
                [
                    item.get("text", "")
                    for item in user_text_plain
                    if item.get("type") == "text"
                ]
            )

        subtitle_keywords = [
            kw.strip() for kw in self.valves.subtitle_keywords.split(",") if kw.strip()
        ]
        is_subtitle_query = any(kw in user_text_plain for kw in subtitle_keywords)
        video_exts = tuple(
            ext.strip().lower()
            for ext in self.valves.subtitle_video_extensions.split(",")
        )

        # 1. Process files list to bypass RAG
        processed_any = False
        if files := body.get("files"):
            files_to_keep = []

            # Parse supported extensions
            supported_exts = tuple(
                ext.strip().lower()
                for ext in self.valves.supported_extensions.split(",")
            )

            for file_item in files:
                # å°è¯•é€‚é…ä¸åŒçš„ files ç»“æ„
                file_obj = file_item.get("file", file_item)

                # è·å–æ–‡ä»¶åå’Œ ID
                filename = file_obj.get("filename") or file_obj.get("name") or ""
                file_id = file_obj.get("id", "")

                print(f"ğŸ“„ Checking file: {filename} (ID: {file_id})")

                # Database-backed deduplication
                file_hash = None
                if openwebui_chat_id and file_id:
                    file_hash = self.get_file_hash(file_id)
                    if file_hash:
                        if self.is_file_analyzed(openwebui_chat_id, file_hash):
                            print(
                                f"â™»ï¸ File {filename} (hash: {file_hash[:8]}...) already analyzed in chat {openwebui_chat_id}, skipping."
                            )
                            continue
                    else:
                        print(
                            f"âš ï¸ Could not find hash for file {file_id}, skipping deduplication check."
                        )

                # Check if extension is supported
                # IMPORTANT: If it's an image, only process if is_target_model is True
                is_image = any(
                    filename.lower().endswith(ext)
                    for ext in [".jpg", ".jpeg", ".png", ".webp", ".gif"]
                )
                is_video = filename.lower().endswith(video_exts)
                should_process = False

                if filename.lower().endswith(supported_exts):
                    if is_image and not is_target_model:
                        print(f"âš ï¸ Skipping image for non-Gemini model: {filename}")
                        should_process = False
                    else:
                        should_process = True

                # Subtitle flow: only if video + subtitle intent
                use_subtitle_flow = should_process and is_video and is_subtitle_query

                if should_process:
                    print(f"ğŸ“„ Detected supported file: {filename}")
                    await self.emit_status(
                        __event_emitter__,
                        f"ğŸš€ å‘ç°æ–°æ–‡ä»¶: {filename}ï¼Œå‡†å¤‡å¤„ç†...",
                        done=False,
                    )

                    # Construct local path in OpenWebUI container
                    file_path = os.path.join(
                        self.valves.openwebui_upload_path, f"{file_id}_{filename}"
                    )

                    upload_success = False
                    try:
                        if os.path.exists(file_path):
                            print(f"ğŸ“– Reading local file: {file_path}")
                            with open(file_path, "rb") as f:
                                file_content = f.read()

                            # Determine mime type
                            mime_type, _ = mimetypes.guess_type(filename)
                            if not mime_type:
                                mime_type = "application/octet-stream"
                                if filename.lower().endswith(".pdf"):
                                    mime_type = "application/pdf"

                            # Upload to Adapter
                            # Upload is fast (local network), so we skip the status update for it to avoid flickering.

                            # Use run_in_executor to avoid blocking the event loop with synchronous requests
                            loop = asyncio.get_running_loop()
                            adapter_url = await loop.run_in_executor(
                                None,
                                self.upload_to_adapter,
                                file_content,
                                filename,
                                mime_type,
                            )

                            if adapter_url:
                                # Mark file as processed in DB
                                if openwebui_chat_id and file_hash:
                                    self.mark_file_analyzed(
                                        openwebui_chat_id, file_hash, file_id, filename
                                    )

                                # MODE SWITCH
                                if effective_mode == "analyzer":
                                    # Analyzer Mode: Analyze first, then inject context
                                    user_text = last_message.get("content", "")
                                    if isinstance(user_text, list):
                                        # Extract text from list content
                                        user_text = " ".join(
                                            [
                                                item["text"]
                                                for item in user_text
                                                if item["type"] == "text"
                                            ]
                                        )

                                    await self.emit_status(
                                        __event_emitter__,
                                        (
                                            "ğŸ§  æ­£åœ¨ä½¿ç”¨å­—å¹•ç²¾ä¿®ä¸“å®¶ç”Ÿæˆé«˜è´¨é‡å­—å¹•..."
                                            if use_subtitle_flow
                                            else "ğŸ§  æ­£åœ¨ä½¿ç”¨ Gemini æ·±åº¦åˆ†ææ–‡æ¡£å†…å®¹ (è¿™å¯èƒ½éœ€è¦å‡ åç§’)..."
                                        ),
                                        done=False,
                                    )
                                    # Give a small yield to ensure status is sent
                                    await asyncio.sleep(0.1)

                                    # Pass openwebui_chat_id to analyze_document
                                    # Also run in executor to avoid blocking
                                    custom_model = (
                                        self.valves.subtitle_custom_model_id
                                        if use_subtitle_flow
                                        else self.valves.analyzer_custom_model_id
                                    )
                                    analysis_result = await loop.run_in_executor(
                                        None,
                                        self.analyze_document,
                                        adapter_url,
                                        user_text,
                                        openwebui_chat_id,
                                        custom_model,
                                        use_subtitle_flow,
                                    )

                                    if analysis_result:
                                        print(
                                            f"âœ… Injecting analysis result into context"
                                        )
                                        # Inject as a system message or prepend to user message
                                        # Prepending to user message is usually safer for context retention
                                        context_title = (
                                            "ã€å­—å¹•ç²¾ä¿®ç»“æœï¼ˆSRT å»ºè®®ï¼‰ã€‘"
                                            if use_subtitle_flow
                                            else "ã€æ–‡æ¡£åˆ†æç»“æœã€‘"
                                        )
                                        context_message = f"{context_title}\n{analysis_result}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n"

                                        content = last_message.get("content", "")
                                        if isinstance(content, str):
                                            last_message["content"] = (
                                                context_message + content
                                            )
                                        elif isinstance(content, list):
                                            # Insert at the beginning of the list
                                            content.insert(
                                                0,
                                                {
                                                    "type": "text",
                                                    "text": context_message,
                                                },
                                            )
                                            last_message["content"] = content

                                        upload_success = True
                                        processed_any = True
                                        await self.emit_status(
                                            __event_emitter__,
                                            (
                                                "âœ… å­—å¹•ç”Ÿæˆå®Œæˆï¼Œå·²æ³¨å…¥ä¸Šä¸‹æ–‡"
                                                if use_subtitle_flow
                                                else "âœ… æ–‡æ¡£åˆ†æå®Œæˆï¼Œå·²æ³¨å…¥ä¸Šä¸‹æ–‡"
                                            ),
                                            done=True,
                                        )
                                    else:
                                        print("âŒ Analysis failed, falling back to RAG")
                                        await self.emit_status(
                                            __event_emitter__,
                                            f"âŒ åˆ†æå¤±è´¥ï¼Œå›é€€åˆ° RAG æ¨¡å¼",
                                            done=True,
                                        )
                                        # upload_success remains False, so it falls back to RAG
                                else:
                                    # Direct Mode: Pass URL to model
                                    print(f"âœ… Adding file to messages: {adapter_url}")
                                    content = last_message.get("content", "")

                                    # Force Subtitle Gem if needed
                                    if use_subtitle_flow:
                                        if not body.get("custom_model_id"):
                                            body["custom_model_id"] = (
                                                self.valves.subtitle_custom_model_id
                                            )
                                        print(
                                            f"ğŸ¬ Subtitle flow detected. Binding custom_model_id -> {body['custom_model_id']}"
                                        )

                                    # Ensure content is a list
                                    if isinstance(content, str):
                                        content = [{"type": "text", "text": content}]

                                    content.append(
                                        {
                                            "type": "image_url",
                                            "image_url": {"url": adapter_url},
                                        }
                                    )
                                    last_message["content"] = content

                                    upload_success = True
                                    processed_any = True
                                    await self.emit_status(
                                        __event_emitter__,
                                        f"ğŸ”— æ–‡ä»¶å·²è¿æ¥åˆ° Gemini æ¨¡å‹ (Direct Mode)",
                                        done=True,
                                    )
                            else:
                                print(f"âŒ Failed to upload file to adapter")
                        else:
                            print(f"âŒ Local file not found: {file_path}")

                    except Exception as e:
                        print(f"âŒ Error processing file {filename}: {e}")
                        await self.emit_status(
                            __event_emitter__, f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}", done=True
                        )

                    # RAG Fallback: If upload failed, keep the file in the list so OpenWebUI handles it
                    # If uploaded successfully, add to keep list (bypass RAG)
                    # OR if we skipped it because it was already processed (in Analyzer mode), we also bypass RAG?
                    # Actually, if we skip analysis, we usually want to bypass RAG too if we rely on Gemini context.
                    # But if we skip, we 'continue'd loop, so we didn't reach here.
                    # Wait, if we continue'd, we didn't add to files_to_keep.
                    # So it will be removed from files list?
                    # Let's check logic:
                    # files_to_keep = []
                    # for file in files:
                    #    if processed: continue
                    #    if success: files_to_keep.append(file)
                    # body['files'] = files_to_keep

                    # If we skip, it's NOT added to files_to_keep. So it's removed from body['files'].
                    # This is CORRECT for Analyzer mode (we don't want RAG).
                    # This is ALSO correct for Direct mode (we don't want RAG).

                    if upload_success:
                        # We processed it, so we remove it from RAG list (by NOT adding to files_to_keep? No wait)
                        # The logic below says:
                        # if upload_success: pass (don't add to files_to_keep) -> removed from RAG
                        # else: files_to_keep.append(file_item) -> kept for RAG
                        pass
                    else:
                        files_to_keep.append(file_item)
                else:
                    # Not a supported file type or image for non-Gemini model, keep it for RAG
                    files_to_keep.append(file_item)

            # Update files list
            # Always update if we have filtered anything, regardless of whether we processed new files
            if len(files_to_keep) != len(files):
                print(
                    f"ğŸš« Updating body['files']: {len(files)} -> {len(files_to_keep)}"
                )
                body["files"] = files_to_keep

        # Handle Analyzer Mode follow-up questions (No new files processed)
        # Only trigger if we are in analyzer mode AND we have processed files in this chat history
        has_processed_files = openwebui_chat_id and self.has_analyzed_files_in_chat(
            openwebui_chat_id
        )

        if effective_mode == "analyzer" and not processed_any and has_processed_files:
            # Check if we have a chat_id (meaning context might exist)
            if openwebui_chat_id:
                print(f"ğŸ¤” Analyzer Mode follow-up detected (has context).")

                user_text = last_message.get("content", "")
                if isinstance(user_text, list):
                    user_text = " ".join(
                        [item["text"] for item in user_text if item["type"] == "text"]
                    )

                # Only analyze if there is text content
                if user_text.strip():
                    await self.emit_status(
                        __event_emitter__,
                        (
                            "ğŸ§  æ­£åœ¨ä½¿ç”¨å­—å¹•ç²¾ä¿®ä¸“å®¶å¤„ç†è¿½é—®..."
                            if is_subtitle_query
                            else "ğŸ§  æ­£åœ¨ä½¿ç”¨ Gemini åˆ†æè¿½é—®..."
                        ),
                        done=False,
                    )

                    loop = asyncio.get_running_loop()
                    followup_custom_model = (
                        self.valves.subtitle_custom_model_id
                        if is_subtitle_query
                        else self.valves.analyzer_custom_model_id
                    )
                    analysis_result = await loop.run_in_executor(
                        None,
                        self.analyze_document,
                        None,
                        user_text,
                        openwebui_chat_id,
                        followup_custom_model,
                        is_subtitle_query,
                    )

                    if analysis_result:
                        print(f"âœ… Injecting follow-up analysis into context")
                        context_title = (
                            "ã€å­—å¹•ç²¾ä¿®ä¸“å®¶å›ç­”ã€‘"
                            if is_subtitle_query
                            else "ã€æ–‡æ¡£åˆ†æåŠ©æ‰‹å›ç­”ã€‘"
                        )
                        context_message = (
                            f"{context_title}\n{analysis_result}\n\nã€ç”¨æˆ·æ–°é—®é¢˜ã€‘\n"
                        )

                        content = last_message.get("content", "")
                        if isinstance(content, str):
                            last_message["content"] = context_message + content
                        elif isinstance(content, list):
                            content.insert(0, {"type": "text", "text": context_message})
                            last_message["content"] = content

                        await self.emit_status(
                            __event_emitter__,
                            f"âœ… è¿½é—®åˆ†æå®Œæˆï¼Œå·²æ³¨å…¥ä¸Šä¸‹æ–‡",
                            done=True,
                        )
                    else:
                        # If analysis fails or returns None (e.g. no session), we just let it pass.
                        # But wait, if analyze_document returns None, it might mean error.
                        # If it's a new chat with no files and no context, analyze_document might be confused?
                        # analyze_document handles file_url=None by using a specific prompt.
                        pass

        # 2. Process existing 'image_url' in content (Base64)
        # Only process if it's the target model (Gemini)
        if is_target_model:
            content = last_message.get("content", "")
            if isinstance(content, list):
                for item in content:
                    if item.get("type") == "image_url":
                        url = item.get("image_url", {}).get("url", "")
                        # Pass should_process_images=True because we already checked is_target_model
                        new_url = self.process_url(url, should_process_images=True)
                        if new_url != url:
                            item["image_url"]["url"] = new_url
                print("âœ… Message content updated with adapter URLs")

        return body

    def outlet(self, body: dict, __user__: dict = None) -> dict:
        return body
