需求文档：异步上下文压缩插件优化 (Async Context Compression Optimization)
1. 核心目标 将现有的基于消息数量的压缩逻辑升级为基于 Token 数量的压缩逻辑，并引入递归摘要机制，以更精准地控制上下文窗口，提高摘要质量，并防止历史信息丢失。

2. 功能需求

Token 计数与阈值控制
引入 tiktoken: 使用 tiktoken 库进行精确的 Token 计数。如果环境不支持，则回退到字符估算 (1 token ≈ 4 chars)。
新配置参数 (Valves):
compression_threshold_tokens (默认: 64000): 当上下文总 Token 数超过此值时，触发压缩（生成摘要）。
max_context_tokens (默认: 128000): 上下文的硬性上限。如果超过此值，强制移除最早的消息（保留受保护消息除外）。
model_thresholds (字典): 支持针对不同模型 ID 配置不同的阈值。例如：{'gpt-4': {'compression_threshold_tokens': 8000, ...}}。
废弃旧参数: compression_threshold (基于消息数) 将被标记为废弃，优先使用 Token 阈值。
递归摘要 (Recursive Summarization)
机制: 在生成新摘要时，必须读取并包含上一次的摘要。
逻辑: 新摘要 = LLM(上一次摘要 + 新产生的对话消息)。
目的: 防止随着对话进行，最早期的摘要信息被丢弃，确保长期记忆的连续性。
消息保护与修剪策略
保护机制: keep_first (保留头部 N 条) 和 keep_last (保留尾部 N 条) 的消息绝对不参与压缩，也不被移除。
修剪逻辑: 当触发 max_context_tokens 限制时，优先移除 keep_first 之后、keep_last 之前的最早消息。
优化的提示词 (Prompt Engineering)
目标: 去除无用信息（寒暄、重复），保留关键信号（事实、代码、决策）。
指令:
提炼与净化: 明确要求移除噪音。
关键保留: 强调代码片段必须逐字保留。
合并与更新: 明确指示将新信息合并到旧摘要中。
语言一致性: 输出语言必须与对话语言保持一致。
3. 实现细节

文件: 
async_context_compression.py
类: 
Filter
关键方法:
_count_tokens(text): 实现 Token 计数。
_calculate_messages_tokens(messages): 计算消息列表总 Token。
_generate_summary_async(...)
: 修改为加载旧摘要，并传入 LLM。
_call_summary_llm(...)
: 更新 Prompt，接受 previous_summary 和 new_messages。
inlet(...)
:
使用 compression_threshold_tokens 判断是否注入摘要。
实现 max_context_tokens 的强制修剪逻辑。
outlet(...)
: 使用 compression_threshold_tokens 判断是否触发后台摘要任务。
