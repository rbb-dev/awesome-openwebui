# 自动上下文合并过滤器 (Auto Context Merger Filter)

## 概述

`auto_context_merger` 是一个 Open WebUI 过滤器插件，旨在通过自动收集和注入上一回合多模型回答的上下文，来增强后续对话的连贯性和深度。当用户在一次多模型回答之后提出新的后续问题时，此过滤器会自动激活。

它会从对话历史中识别出上一回合所有 AI 模型的回答，将它们按照清晰的格式直接拼接起来，然后作为一个系统消息注入到当前请求中。这样，当前模型在处理用户的新问题时，就能直接参考到之前所有 AI 的观点，从而提供更全面、更连贯的回答。

## 工作原理

1.  **触发时机**: 当用户在一次“多模型回答”之后，发送新的后续问题时，此过滤器会自动激活。
2.  **获取历史数据**: 过滤器会使用当前对话的 `chat_id`，从数据库中加载完整的对话历史记录。
3.  **分析上一回合**: 通过分析对话树结构，它能准确找到用户上一个问题，以及当时所有 AI 模型给出的并行回答。
4.  **直接格式化**: 如果检测到上一回合确实有多个 AI 回答，它会收集所有这些 AI 的回答内容。
5.  **智能注入**: 将这些格式化后的回答作为一个系统消息，注入到当前请求的 `messages` 列表的开头，紧邻用户的新问题之前。
6.  **传递给目标模型**: 修改后的消息体（包含格式化后的上下文）将传递给用户最初选择的目标模型。目标模型在生成响应时，将能够利用这个更丰富的上下文。
7.  **状态更新**: 在整个处理过程中，过滤器会通过 `__event_emitter__` 提供实时状态更新，让用户了解处理进度。

## 配置 (Valves)

您可以在 Open WebUI 的管理界面中配置此过滤器的 `Valves`。

*   **`CONTEXT_PREFIX`** (字符串, 必填):
    *   **描述**: 注入的系统消息的前缀文本。它会出现在合并后的上下文之前，用于向模型解释这段内容的来源和目的。
    *   **示例**: `**背景知识**：为了更好地回答您的新问题，请参考上一轮对话中多个AI模型给出的回答：\n\n`

## 如何使用

1.  **部署过滤器**: 将 `auto_context_merger.py` 文件放置在 Open WebUI 实例的 `plugins/filters/` 目录下。
2.  **启用过滤器**: 登录 Open WebUI 管理界面，导航到 **Workspace -> Functions**。找到 `auto_context_merger` 过滤器并启用它。
3.  **配置参数**: 点击 `auto_context_merger` 过滤器旁边的编辑按钮，根据您的需求配置 `CONTEXT_PREFIX`。
4.  **开始对话**:
    *   首先，向一个模型提问，并确保有多个模型（例如通过 `gemini_manifold` 或其他多模型工具）给出回答。
    *   然后，针对这个多模型回答，提出您的后续问题。
    *   此过滤器将自动激活，将上一回合所有 AI 的回答合并并注入到当前请求中。

## 示例

假设您配置了 `CONTEXT_PREFIX` 为默认值。

1.  **用户提问**: “解释一下量子力学”
2.  **多个 AI 回答** (例如，模型 A 和模型 B 都给出了回答)
3.  **用户再次提问**: “那么，量子纠缠和量子隧穿有什么区别？”

此时，`auto_context_merger` 过滤器将自动激活：
1.  它会获取模型 A 和模型 B 对“解释一下量子力学”的回答。
2.  将它们格式化为：
    ```
    **背景知识**：为了更好地回答您的新问题，请参考上一轮对话中多个AI模型给出的回答：

    **来自模型 '模型A名称' 的回答是：**
    [模型A对量子力学的解释]

    ---

    **来自模型 '模型B名称' 的回答是：**
    [模型B对量子力学的解释]
    ```
3.  然后，将这段内容作为一个系统消息，注入到当前请求中，紧邻“那么，量子纠缠和量子隧穿有什么区别？”这个用户问题之前。

最终，模型将收到一个包含所有相关上下文的请求，从而能够更准确、更全面地回答您的后续问题。

## 注意事项

*   此过滤器旨在增强多模型对话的连贯性，通过提供更丰富的上下文来帮助模型理解后续问题。
*   确保您的 Open WebUI 实例中已配置并启用了 `gemini_manifold` 或其他能够产生多模型回答的工具，以便此过滤器能够检测到多模型历史。
*   此过滤器不会增加额外的模型调用，因此不会显著增加延迟或成本。它只是对现有历史数据进行格式化和注入。
